import { readFile, writeFile, mkdir, readdir, stat } from "fs/promises";
import { join, resolve, extname, basename } from "path";
import { extractChunksFromDocument } from "./llm-client.js";
import type { GuideEntry, LLMChunkOutput } from "./schemas.js";
import {
  loadManifest,
  saveManifest,
  hashBuffer,
  recordExtraction,
  getChunkIdsForSource,
} from "./scripts/source-manifest.js";
import {
  decodePdfToText,
  segmentDocument,
  logSegmentSummary,
  type DocumentSegment,
} from "./chunker.js";
import { CONFIG } from "./config.js";

// â”€â”€â”€ Markdown chunk assembler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Converts a validated LLMChunkOutput into the canonical .md format defined
// in the architecture spec. Front matter is YAML, body has fixed sections.

function assembleChunkMarkdown(chunk: LLMChunkOutput): string {
  const lines: string[] = [];

  // â”€â”€ YAML front matter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  lines.push("---");
  lines.push(`chunk_id: ${chunk.chunk_id}`);
  lines.push(`topic: ${chunk.topic}`);

  // Multi-line summary uses YAML block scalar
  lines.push(`summary: >`);
  lines.push(`  ${chunk.summary}`);

  lines.push("triggers:");
  for (const trigger of chunk.triggers) {
    lines.push(`  - "${trigger.replace(/"/g, "'")}"`);
  }

  lines.push(`has_conditions: ${chunk.has_conditions}`);

  if (chunk.escalation) {
    lines.push(`escalation: "${chunk.escalation.replace(/"/g, "'")}"`);
  } else {
    lines.push("escalation: null");
  }

  lines.push("related_chunks:");
  for (const rel of chunk.related_chunks) {
    lines.push(`  - ${rel}`);
  }

  lines.push(`status: ${chunk.status}`);
  lines.push("---");
  lines.push("");

  // â”€â”€ Context â€” always present â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  lines.push("## Context");
  lines.push("");
  lines.push(chunk.context.trim());
  lines.push("");

  // â”€â”€ Conditions â€” only when has_conditions: true â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  if (chunk.has_conditions && chunk.conditions) {
    lines.push("## Conditions");
    lines.push("");
    lines.push(chunk.conditions.trim());
    lines.push("");
  }

  // â”€â”€ Constraints â€” only when hard limits exist â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  if (chunk.constraints) {
    lines.push("## Constraints");
    lines.push("");
    lines.push(chunk.constraints.trim());
    lines.push("");
  }

  // â”€â”€ Response â€” always present for active chunks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  lines.push("## Response");
  lines.push("");
  lines.push(chunk.response.trim());
  lines.push("");

  // â”€â”€ Escalation â€” always present â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  lines.push("## Escalation");
  lines.push("");
  lines.push(chunk.escalation_detail.trim());
  lines.push("");

  // â”€â”€ Image descriptions â€” appended if present â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  // These are not customer-facing but are stored in the chunk for future use
  // (e.g. generating alt text, grounding answers with visual context).
  if (chunk.image_descriptions && chunk.image_descriptions.length > 0) {
    lines.push("## Images");
    lines.push("");
    for (const img of chunk.image_descriptions) {
      lines.push(`### ${img.caption || "Unnamed image"}`);
      lines.push("");
      lines.push(`**Position:** ${img.position_hint}`);
      lines.push("");
      lines.push(`**Description:** ${img.full_description}`);
      lines.push("");
      lines.push(`**Relevance:** ${img.relevance}`);
      lines.push("");
    }
  }

  return lines.join("\n");
}

// â”€â”€â”€ Guide YAML helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const GUIDE_PATH = CONFIG.paths.guide;

async function loadGuide(): Promise<GuideEntry[]> {
  try {
    const raw = await readFile(GUIDE_PATH, "utf-8");
    const entries: GuideEntry[] = [];

    // Parse YAML blocks split by chunk_id markers
    const blocks = raw
      .split(/^  - chunk_id:/m)
      .filter((b) => b.trim() && !b.trim().startsWith("#"));

    for (const block of blocks) {
      try {
        const chunk_id = block.match(/^\s*([^\n]+)/)?.[1]?.trim() ?? "";
        const topic = block.match(/\n\s+topic:\s*(.+)/)?.[1]?.trim() ?? "";
        const summary =
          block.match(/summary:\s*>\s*\n\s+(.+)/)?.[1]?.trim() ?? "";
        const file = block.match(/\n\s+file:\s*(.+)/)?.[1]?.trim() ?? "";
        const has_conditions =
          block.match(/\n\s+has_conditions:\s*(true|false)/)?.[1] === "true";
        const escalationRaw =
          block.match(/\n\s+escalation:\s*(.+)/)?.[1]?.trim() ?? "null";
        const escalation =
          escalationRaw === "null" ? null : escalationRaw.replace(/^"|"$/g, "");
        const status = (block.match(/\n\s+status:\s*(\w+)/)?.[1]?.trim() ??
          "active") as "active" | "review" | "deprecated";

        // Parse triggers â€” collect lines between "triggers:" and next key
        const triggersSection = block.match(
          /\n\s+triggers:\s*\n((?:\s+- .+\n?)*)/,
        );
        const triggers = triggersSection?.[1]
          ? [...triggersSection[1].matchAll(/- "?(.+?)"?\s*$/gm)].map((m) =>
              m[1]!.trim(),
            )
          : [];

        // Parse related_chunks â€” same approach
        const relatedSection = block.match(
          /\n\s+related_chunks:\s*\n((?:\s+- .+\n?)*)/,
        );
        const related_chunks = relatedSection?.[1]
          ? [...relatedSection[1].matchAll(/- (.+?)\s*$/gm)].map((m) =>
              // Normalize: strip 'chunk_id:' prefixes (GAP-D1-05)
              m[1]!.trim().replace(/^chunk_id:/i, ""),
            )
          : [];

        if (chunk_id && topic) {
          entries.push({
            chunk_id,
            topic,
            summary,
            triggers,
            has_conditions,
            escalation,
            related_chunks,
            status,
            file,
          });
        }
      } catch {
        // skip malformed block
      }
    }

    return entries;
  } catch {
    return [];
  }
}

/**
 * Serialize all guide entries to guide.yaml.
 * The format mirrors the spec exactly â€” extracted from chunk front matter.
 */
async function saveGuide(entries: GuideEntry[]): Promise<void> {
  const lines: string[] = [
    "# Knowledge Base Guide Index",
    "# Auto-generated from chunk front matter â€” do not edit manually",
    "# Source of truth: individual chunk .md files in data/chunks/",
    "",
    "chunks:",
    "",
  ];

  for (const entry of entries) {
    lines.push(`  - chunk_id: ${entry.chunk_id}`);
    lines.push(`    topic: ${entry.topic}`);
    lines.push(`    summary: >`);
    lines.push(`      ${entry.summary}`);

    lines.push(`    triggers:`);
    for (const trigger of entry.triggers) {
      lines.push(`      - "${trigger.replace(/"/g, "'")}"`);
    }

    lines.push(`    has_conditions: ${entry.has_conditions}`);

    if (entry.escalation) {
      lines.push(`    escalation: "${entry.escalation.replace(/"/g, "'")}"`);
    } else {
      lines.push(`    escalation: null`);
    }

    lines.push(`    related_chunks:`);
    for (const rel of entry.related_chunks) {
      lines.push(`      - ${rel}`);
    }

    lines.push(`    status: ${entry.status}`);
    lines.push(`    file: ${entry.file}`);
    lines.push("");
  }

  await writeFile(GUIDE_PATH, lines.join("\n"), "utf-8");
  console.log(`\nğŸ“˜ guide.yaml updated â€” ${entries.length} chunk(s)\n`);
}

// â”€â”€â”€ Guide context-window size guard (GAP-D1-19) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const MAX_CHUNKS_BEFORE_WARNING = 80;
const MAX_GUIDE_KB_BEFORE_WARNING = 50;

async function checkContextWindowSize(
  guidePath: string,
  currentCount: number,
): Promise<void> {
  if (currentCount <= MAX_CHUNKS_BEFORE_WARNING) return;

  let fileKb = 0;
  try {
    const info = await stat(guidePath);
    fileKb = info.size / 1024;
  } catch {
    /* ignore */
  }

  if (
    currentCount > MAX_CHUNKS_BEFORE_WARNING ||
    fileKb > MAX_GUIDE_KB_BEFORE_WARNING
  ) {
    console.warn(`
âš ï¸  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸  CONTEXT WINDOW SIZE WARNING (GAP-D1-19)
âš ï¸
âš ï¸  Knowledge base now has ${currentCount} chunks (${fileKb.toFixed(1)} KB).
âš ï¸  Retrieval sends the full guide.yaml to the LLM on every query.
âš ï¸  At this scale, you risk exceeding the LLM context window.
âš ï¸
âš ï¸  Recommended: migrate to embedding-based retrieval (vector store).
âš ï¸  See: https://sdk.vercel.ai/docs/ai-sdk-core/embeddings
âš ï¸  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
`);
  }
}

async function readPdf(
  filePath: string,
): Promise<{ base64: string; buf: Buffer }> {
  console.log(`ğŸ“„ Reading PDF: ${filePath}`);
  const buf = await readFile(filePath);
  return { base64: buf.toString("base64"), buf };
}

// â”€â”€â”€ Extraction strategies â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Fallback: original single-shot LLM extraction.
 * Used when pdf-parse can't extract text (image-only PDFs).
 * Non-deterministic â€” the LLM decides boundaries and IDs itself.
 */
async function fallbackExtract(
  base64: string,
  source: string,
  extractionType: "procedure" | "qna" = "procedure",
): Promise<LLMChunkOutput[]> {
  const label = `â±  LLM extraction (single-shot) [${basename(source)}]`;
  console.time(label);
  const chunks = await extractChunksFromDocument(
    base64,
    undefined,
    extractionType,
  );
  console.timeEnd(label);
  return chunks;
}

/**
 * Deterministic segment-level extraction (GAP-D1-01).
 *
 * For each pre-bounded segment from segmentDocument():
 *   1. Build a per-segment prompt containing the section text.
 *   2. Call LLM with segment prompt + full PDF (for image context).
 *   3. OVERRIDE the LLM's chunk_id with our stable deriveChunkId() ID.
 *
 * Result: same PDF â†’ same segments â†’ same IDs every time.
 * LLM non-determinism only affects wording, not boundaries or IDs.
 */
async function extractFromSegments(
  segments: DocumentSegment[],
  base64: string,
  source: string,
  extractionType: "procedure" | "qna" = "procedure",
): Promise<LLMChunkOutput[]> {
  const { deriveChunkId } = await import("./chunker.js");
  const allChunks: LLMChunkOutput[] = [];

  console.log(
    `\nğŸ“‹ Extracting ${segments.length} segment(s) individually (${extractionType} mode)...\n`,
  );

  for (const seg of segments) {
    const label = `â±  segment [${seg.stableChunkId}]`;
    console.time(label);

    let segmentPrompt: string;

    if (extractionType === "qna") {
      segmentPrompt =
        `You are extracting Q&A pairs/FAQs from a specific section of a PDF.\n\n` +
        `SECTION HEADING: ${seg.headingPath.join(" â€º ")}\n` +
        `SECTION PAGES: ${seg.pageRange.start}â€“${seg.pageRange.end}\n\n` +
        `SECTION TEXT:\n${seg.content}\n\n---\n\n` +
        `Extract ONLY valid Questions and Answers found in this section.\n` +
        `Required fields for each Q&A chunk: chunk_id, topic, summary, triggers (the question), ` +
        `has_conditions, escalation, related_chunks, status, context, response (the answer), image_descriptions.\n` +
        `Return ONLY a raw JSON array. Start with [ and end with ]. No markdown fences.`;
    } else {
      segmentPrompt =
        `You are extracting from a specific section of a PDF document.\n\n` +
        `SECTION HEADING: ${seg.headingPath.join(" â€º ")}\n` +
        `SECTION PAGES: ${seg.pageRange.start}â€“${seg.pageRange.end}\n\n` +
        `SECTION TEXT:\n${seg.content}\n\n---\n\n` +
        `Extract the knowledge in this section only. Produce a SINGLE chunk JSON object (not an array).\n` +
        `Required fields: chunk_id, topic, summary, triggers, has_conditions, escalation, ` +
        `related_chunks, status, context, response, escalation_detail, image_descriptions.\n` +
        `Return ONLY valid JSON. No markdown fences. No explanation.`;
    }

    try {
      const chunks = await extractChunksFromDocument(
        base64,
        segmentPrompt,
        extractionType,
      );
      console.timeEnd(label);

      for (const chunk of chunks) {
        // Override LLM-generated ID with our deterministic content-hash ID
        // Note: For QnA, if multiple Q&A's in one segment, we append a suffix
        const baseStableId = deriveChunkId(seg.headingPath, chunk.topic);
        const stableId =
          chunks.length > 1
            ? `${baseStableId}-${chunks.indexOf(chunk)}`
            : baseStableId;

        console.log(`  âœ“ ${stableId}  (LLM suggested: ${chunk.chunk_id})`);
        allChunks.push({ ...chunk, chunk_id: stableId });
      }
    } catch (err) {
      console.timeEnd(label);
      console.warn(
        `  âš ï¸  Segment "${seg.stableChunkId}" extraction failed:`,
        err,
      );
    }
  }

  return allChunks;
}

// â”€â”€â”€ Core extraction pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function extractSingle(
  source: string,
  outputDir: string,
  manifest: Awaited<ReturnType<typeof loadManifest>>,
  extractionType: "procedure" | "qna" = "procedure",
): Promise<{
  saved: number;
  newCount: number;
  updatedCount: number;
  chunkIds: string[];
}> {
  const { base64: content, buf } = await readPdf(source);
  const currentHash = hashBuffer(buf);

  console.log(`  â†³ PDF size: ${(buf.length / 1024).toFixed(1)} KB\n`);

  // â”€â”€ Step 1: Extract plain text for deterministic segmentation (GAP-D1-01) â”€â”€â”€â”€
  let chunks: LLMChunkOutput[];

  const pdfText = await decodePdfToText(content);

  if (pdfText.length > CONFIG.extraction.minTextLengthForSegmentation) {
    // Text layer available â€” use deterministic segment-per-LLM-call pipeline
    const docTitle = basename(source).replace(/\.pdf$/i, "");
    const segments = segmentDocument(pdfText, docTitle);
    logSegmentSummary(segments);

    if (segments.length === 0) {
      console.log(
        "  âš ï¸  Segmenter produced 0 segments. Falling back to single-shot extraction.\n",
      );
      chunks = await fallbackExtract(content, source, extractionType);
    } else {
      chunks = await extractFromSegments(
        segments,
        content,
        source,
        extractionType,
      );
    }
  } else {
    // Image-only PDF: no text layer â€” fall back to single-shot LLM extraction
    console.log(
      "  âš ï¸  No text layer found (image-only PDF?). Using single-shot LLM extraction.\n",
    );
    chunks = await fallbackExtract(content, source, extractionType);
  }

  if (chunks.length === 0) {
    console.log("  âš ï¸  No chunks extracted from this document.\n");
    return { saved: 0, newCount: 0, updatedCount: 0, chunkIds: [] };
  }

  // â”€â”€ Step 2: Save chunks and update guide â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  const guide = await loadGuide();
  let savedCount = 0;
  let newCount = 0;
  let updatedCount = 0;
  const savedChunkIds: string[] = [];

  for (const chunk of chunks) {
    try {
      const fileName = `${chunk.chunk_id}.md`;
      const filePath = join(outputDir, fileName);
      const relPath = `data/chunks/${fileName}`;

      const markdown = assembleChunkMarkdown(chunk);
      await writeFile(filePath, markdown, "utf-8");

      const existingIdx = guide.findIndex((e) => e.chunk_id === chunk.chunk_id);
      const entry: GuideEntry = {
        chunk_id: chunk.chunk_id,
        topic: chunk.topic,
        summary: chunk.summary,
        triggers: chunk.triggers,
        has_conditions: chunk.has_conditions,
        escalation: chunk.escalation,
        related_chunks: chunk.related_chunks,
        status: chunk.status,
        file: relPath,
      };

      if (existingIdx >= 0) {
        guide[existingIdx] = entry;
        console.log(`  â†» Updated: ${fileName}`);
        updatedCount++;
      } else {
        guide.push(entry);
        console.log(`  âœ“ Created: ${fileName}`);
        newCount++;
      }

      console.log(`    Topic:      ${chunk.topic}`);
      console.log(`    Summary:    ${chunk.summary}`);
      console.log(`    Triggers:   ${chunk.triggers.length}`);
      console.log(`    Images:     ${chunk.image_descriptions.length}`);
      console.log(`    Conditions: ${chunk.has_conditions}`);
      console.log("");

      savedCount++;
      savedChunkIds.push(chunk.chunk_id);
    } catch (error) {
      console.error(`  âœ— Failed to save chunk "${chunk.chunk_id}":`, error);
    }
  }

  await saveGuide(guide);

  // Context window size guard (GAP-D1-19)
  await checkContextWindowSize(GUIDE_PATH, guide.length);

  // Update source manifest (GAP-D1-14 / GAP-D1-17)
  recordExtraction(manifest, source, currentHash, buf.length, savedChunkIds);

  return { saved: savedCount, newCount, updatedCount, chunkIds: savedChunkIds };
}

// â”€â”€â”€ Input resolution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function resolveSources(args: string[]): Promise<string[]> {
  const sources: string[] = [];

  for (const arg of args) {
    const resolved = resolve(arg);
    const info = await stat(resolved);

    if (info.isDirectory()) {
      const entries = await readdir(resolved);
      const pdfs = entries
        .filter((f) => extname(f).toLowerCase() === ".pdf")
        .sort()
        .map((f) => join(resolved, f));

      if (pdfs.length === 0) {
        console.warn(`âš ï¸  No PDF files found in directory: ${resolved}`);
      } else {
        console.log(`ğŸ“‚ Found ${pdfs.length} PDF(s) in ${resolved}\n`);
        sources.push(...pdfs);
      }
    } else if (info.isFile()) {
      if (extname(resolved).toLowerCase() !== ".pdf") {
        console.warn(`âš ï¸  Skipping non-PDF file: ${arg}`);
      } else {
        sources.push(resolved);
      }
    } else {
      console.warn(`âš ï¸  Skipping unknown path: ${arg}`);
    }
  }

  return sources;
}

// â”€â”€â”€ CLI Entry Point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function main() {
  const args = process.argv.slice(2);

  if (args.length === 0) {
    console.log(`
Usage:
  bun run extract [options] <source> [source2] ...

Options:
  --type=procedure  Extract standard procedures (default)
  --type=qna        Extract Q&A pairs / FAQs

Sources:
  â€¢ A single PDF file      bun run extract ./manual.pdf
  â€¢ Multiple PDFs          bun run extract a.pdf b.pdf
  â€¢ A directory (all PDFs) bun run extract ./docs/
  â€¢ Mixed                  bun run extract --type=qna faq.pdf

Tip: For a full ingestion pipeline (extract â†’ validate â†’ relate â†’ rebuild),
     use: bun run ingest <sources>
`);
    process.exit(1);
  }

  const typeFlag = args.find((a) => a.startsWith("--type="));
  const extractionType: "procedure" | "qna" =
    typeFlag === "--type=qna" ? "qna" : "procedure";
  const sourcesArgs = args.filter((a) => !a.startsWith("--type="));

  try {
    const sources = await resolveSources(sourcesArgs);

    if (sources.length === 0) {
      console.error("âŒ No valid PDF sources found.");
      process.exit(1);
    }

    console.log(
      `\nğŸš€ Starting extraction for ${sources.length} source(s)...\n`,
    );

    const outputDir = CONFIG.paths.chunks;
    await mkdir(outputDir, { recursive: true });
    await mkdir(CONFIG.paths.data, { recursive: true });

    // â”€â”€ Load source manifest (GAP-D1-14 / GAP-D1-17) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const manifest = await loadManifest();

    // â”€â”€ Per-source extraction with summary tracking (GAP-D1-18) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    let totalNew = 0;
    let totalUpdated = 0;
    let totalFailed = 0;
    const extractStart = Date.now();

    for (let i = 0; i < sources.length; i++) {
      const source = sources[i]!;
      const label = basename(source);
      console.log(`\nâ”â”â” [${i + 1}/${sources.length}] ${label} â”â”â”\n`);

      try {
        const result = await extractSingle(
          source,
          outputDir,
          manifest,
          extractionType,
        );
        totalNew += result.newCount;
        totalUpdated += result.updatedCount;
      } catch (err) {
        console.error(`âŒ Failed to extract from ${label}:`, err);
        totalFailed++;
      }
    }

    // â”€â”€ Save updated manifest (GAP-D1-17) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    await saveManifest(manifest);
    console.log(`\nğŸ“‹ source-manifest.json updated`);

    // â”€â”€ Extraction summary report (GAP-D1-18) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const totalElapsed = ((Date.now() - extractStart) / 1000).toFixed(1);
    const totalSaved = totalNew + totalUpdated;

    console.log(`
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š Extraction Summary
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
   Sources processed : ${sources.length}
   Chunks created    : ${totalNew}
   Chunks updated    : ${totalUpdated}
   Sources failed    : ${totalFailed}
   Total time        : ${totalElapsed}s
   Output directory  : ${outputDir}
   Guide index       : data/guide.yaml
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Next steps:
  1. Validate chunks:  bun run validate
  2. Link related:     bun run relate
  3. Rebuild index:    bun run rebuild
  â€” or run all steps: bun run ingest <sources>
`);
  } catch (error) {
    console.error("Extraction failed:", error);
    process.exit(1);
  }
}

main();
