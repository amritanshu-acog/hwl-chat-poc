# HWL Troubleshooting Assistant

An LLM-powered helpdesk assistant that extracts knowledge from PDFs and deterministically answers user questions based **only** on that indexed knowledge.

## ðŸš€ Quick Start

1. `cp .env.example .env` (Add your LLM API keys)
2. `bun install`
3. `bun run ingest <path/to/pdf>` (Extracts, validates, and indexes your document)
4. `bun run server` (Starts API on `localhost:3000`)
5. `bun run chat` (Interactive terminal testing)

## ðŸ§  Architecture Overview

- **Offline (Ingestion)**: PDFs â†’ `bun run ingest` â†’ Markdown chunks in `/chunks` + `guide.yaml` index.
- **Online (Chat)**: User query â†’ AI retrieves 2-3 specific chunks â†’ AI generates a strict JSON UI response based _only_ on those chunks.

## ðŸ’» Core Commands

- `bun run ingest <path>` â€” The full pipeline: extracts, validates, relates, and rebuilds the index.
- `bun run server` â€” Runs the HTTP API server.
- `bun run chat [--debug]` â€” CLI chat (use `--debug` to see the exact evidence chunks the AI read).
- `bun run score` â€” Evaluates retrieval accuracy against `data/test-queries.json`.
- `bun run e2e-test` â€” Instant structural integrity regression check of the knowledge base.
- `bun run chunk <pdf>` â€” Debug tool to visually preview how a PDF is chunked before LLM extraction.

_(For a complete list of all 12 scripts, read [scripts.md](./scripts.md))_
_(For deeply detailed technical documentation, read [HELP.md](./HELP.md))_

## ðŸ”Œ API Integration (`POST /api/chat`)

Send questions to the system to get typed JSON components (UI elements) and file citations back:

```json
// Request
{
  "message": "How do I reset my password?",
  "sessionId": "user-123"
}

// Response
{
  "response": {
    "type": "steps",
    "data": { "title": "Reset Password", "steps": [...] }
  },
  "contextChunks": [
    {
       "chunk_id": "hwl-agency-password-reset",
       "topic": "Password Management",
       "file": "HWL Agency_Staff Pool V3.pdf"
    }
  ]
}
```
